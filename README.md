# ðŸŒŸ Exploring Hugging Face Transformers ðŸš€

This repository is my **first hands-on exploration** of Hugging Faceâ€™s **Transformers library** and its **tokenizers**.  
The goal is to get familiar with how modern NLP models work, try out a few popular architectures, and understand how easily Hugging Face lets us switch between tasks like **text generation, sentiment analysis, and translation/summarization**.  

---

## ðŸ“‚ Contents
- `Exploring Generative AI Libraries-v2.ipynb` â†’ Main Jupyter Notebook with all the experiments and outputs.  
- `README.md` â†’ This documentation.  

---

## ðŸ¤– Models Explored
I experimented with **three diverse Hugging Face models**:

1. **GPT-2 (`gpt2`)** â†’ Text Generation  
   - Generates coherent and creative text.  
   - Example: Story continuation, creative writing, idea generation.  

2. **DistilBERT (`distilbert-base-uncased-finetuned-sst-2-english`)** â†’ Sentiment Analysis  
   - A lightweight and fast version of BERT.  
   - Example: Detect if a sentence is **Positive** or **Negative**.  

3. **T5-small (`t5-small`)** â†’ Translation & Summarization  
   - A sequence-to-sequence model.  
   - Example: Translate between languages or summarize long text.  

ðŸ‘‰ Together, these models cover **generation**, **classification**, and **seq2seq** tasks, giving a broad taste of Hugging Faceâ€™s ecosystem.  

---

## âš¡ Quick Start
### ðŸ”§ Installation
You only need two libraries:  
```bash
pip install transformers torch
